<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Tips for Specifying Priors • stray</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Tips for Specifying Priors">
<meta property="og:description" content="stray">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">stray</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.13</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="https://github.com/jsilve24/stray/wiki/Installation-Details">Installation</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Vignettes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/introduction-to-stray.html">Intro to stray through stray::pibble</a>
    </li>
    <li>
      <a href="../articles/non-linear-models.html">Non-Linear Modeling with stray::basset</a>
    </li>
    <li>
      <a href="../articles/orthus.html">Joint Modeling  (e.g., Multiomics) with Stray::Orthus</a>
    </li>
    <li>
      <a href="../articles/picking_priors.html">Tips on Specifying Priors</a>
    </li>
  </ul>
</li>
<li>
  <a href="../reference/index.html">Functions</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Other Packages
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="https://github.com/jsilve24/RcppCoDA">RcppCoDA</a>
    </li>
    <li>
      <a href="https://jsilve24.github.io/driver/">driver</a>
    </li>
    <li>
      <a href="https://bioconductor.org/packages/release/bioc/html/philr.html">philr</a>
    </li>
    <li>
      <a href="https://cran.r-project.org/package=RcppHungarian">RcppHungarian</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://twitter.com/inschool4life">
    <span class="fa fa-twitter"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/jsilve24/stray">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="picking_priors_files/header-attrs-2.2/header-attrs.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Tips for Specifying Priors</h1>
            
      
      
      <div class="hidden name"><code>picking_priors.Rmd</code></div>

    </div>

    
    
<div id="overview" class="section level1">
<h1 class="hasAnchor">
<a href="#overview" class="anchor"></a>Overview</h1>
<p>Picking priors is both an important and difficult part of Bayesian statistics. This vignette is not intended to be an introduction to Bayesian statistics, here I assume readers are already know what a prior/posterior is. Just to review, a prior is a probability distribution representing an analysts belief in model parameters prior to seeing the data. The posterior is the (in some sense optimal) probability distribution representing what you should belief having seen the data (given your prior beliefs).</p>
<p>Since priors represent an analysts belief prior to seeing the data, it makes sense that priors will often be specific for a given study. For example, we don’t necessarily believe the parameters learned for an RNA-seq data analysis will be the same as for someone studying microbial communities or political gerrymandering. What’s more, we probably have different prior beliefs depending on what microbial community we are studying or how a study is set up.</p>
<p>There are (at least) two important reasons to think carefully about priors. First, the meaning of the posterior is conditioned on your prior accurately reflecting your beliefs. The posterior represents an optimal belief given data and <em>given prior beleifs</em>. If a specified prior does not reflect your beliefs well then the prior won’t have the right meaning. Of course all priors are imperfect but we do the best we can. Second, on a practical note, some really weird priors can lead to numerical issues during optimization and uncertainty quantification in <em>stray</em>. This later problem can appear as failure to reach the MAP estimate or an error when trying to invert the Hessian.</p>
<p>Overall, a prior is a single function (a probability distribution) specified jointly on parameters of interest. Still, it can be confusing to think about the prior in the joint form. Here I will instead try to simplify this and break the prior down into distinct components. While there are numerous models in <em>stray</em>, here I will focus on the prior for the <em>pibble</em> model as it is, in my opinion, the heart of <em>stray</em>.</p>
<p>Just to review, the pibble model is given by: <span class="math display">\[
\begin{align}
Y_j &amp; \sim \text{Multinomial}\left(\pi_j \right)  \\
\pi_j &amp; = \phi^{-1}(\eta_j) \\
\eta_j &amp;\sim N(\Lambda X_j, \Sigma) \\
\Lambda &amp;\sim  N(\Theta, \Sigma, \Gamma) \\
\Sigma &amp;\sim W^{-1}(\Xi, \upsilon). 
\end{align}
\]</span> We consider the first two lines to be part of the likelihood and the bottom three lines to be part of the prior. Therefore we have the following three components of the prior:</p>
<ul>
<li>The prior for <span class="math inline">\(\Sigma\)</span>: <span class="math inline">\(\Sigma \sim W^{-1}(\Xi, \upsilon)\)</span>
</li>
<li>The prior for <span class="math inline">\(\Lambda\)</span>: <span class="math inline">\(\Lambda \sim N(\Theta, \Sigma, \Gamma)\)</span>
</li>
<li>The prior for <span class="math inline">\(\eta_j\)</span>: <span class="math inline">\(\eta_j \sim N(\Lambda X_j, \Sigma)\)</span>
</li>
</ul>
</div>
<div id="background-on-the-matrix-normal" class="section level1">
<h1 class="hasAnchor">
<a href="#background-on-the-matrix-normal" class="anchor"></a>Background on the Matrix Normal</h1>
<p>There are three things I should explain before going forward. The vec operation, the Kronecker product, and the matrix normal. The first two are needed to understand the matrix-normal.</p>
<div id="the-vec-operation" class="section level2">
<h2 class="hasAnchor">
<a href="#the-vec-operation" class="anchor"></a>The Vec Operation</h2>
<p>The vec operation is just a special way of saying column stacking. If we have a<br>
matrix <span class="math display">\[X = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\]</span> then <span class="math display">\[vec(X) = \begin{bmatrix} a\\ c \\ b\\d\end{bmatrix}.\]</span> It’s that simple.</p>
</div>
<div id="kronker-products" class="section level2">
<h2 class="hasAnchor">
<a href="#kronker-products" class="anchor"></a>Kronker Products</h2>
<p>It turns out there are many different definitions for how to multiply two matrices together. There is standard matrix multiplication, there is element-wise multiplication, there is also something called the Kronecker product. Given two matrices <span class="math inline">\(X = \begin{bmatrix} x_{11} &amp; x_{12} \\ x_{21} &amp; x_{22} \end{bmatrix}\)</span> and <span class="math inline">\(Y = \begin{bmatrix} y_{11} &amp; y_{12} \\ y_{21} &amp; y_{22} \end{bmatrix}\)</span>, we define the Kronecker product of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> as <span class="math display">\[
X \otimes Y = \begin{bmatrix}x_{11}Y &amp; x_{12}Y \\ x_{21}Y &amp; x_{22}Y \end{bmatrix} = 
\begin{bmatrix} 
x_{11}y_{11} &amp; x_{11}y_{12} &amp; x_{12}y_{11} &amp; x_{12}y_{12} \\
x_{11}y_{21} &amp; x_{11}y_{22} &amp; x_{12}y_{21} &amp; x_{12}y_{22} \\                
x_{21}y_{11} &amp; x_{21}y_{12} &amp; x_{22}y_{11} &amp; x_{22}y_{12} \\
x_{21}y_{21} &amp; x_{21}y_{22} &amp; x_{22}y_{21} &amp; x_{22}y_{22} \\  
\end{bmatrix}.
\]</span> Notice how we are essentially making a larger matrix by patterning Y over X?</p>
</div>
<div id="the-matrix-normal" class="section level2">
<h2 class="hasAnchor">
<a href="#the-matrix-normal" class="anchor"></a>The Matrix Normal</h2>
<p>Before going forward you may be wondering why the normal in the prior for <span class="math inline">\(\Lambda\)</span> has three parameters (<span class="math inline">\(\Theta\)</span>, <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(\Gamma\)</span>) rather than two. This means that our prior for <span class="math inline">\(\Lambda\)</span> is a <em>matrix normal</em> rather than a <em>multivariate normal</em>. The matrix normal is a generalization of the multivariate normal for random matrices (not just random vectors). Below is a simplified description of the matrix normal.</p>
<p>With the multivariate normal you have a mean vector and a covariance matrix describing the spread of the distribution about the mean. With the matrix normal you have a mean matrix, and two covariance matrices describing the spread of the distribution about the mean. The first covariance matrix (<span class="math inline">\(\Sigma\)</span>) describes the covariance between the rows of <span class="math inline">\(\Lambda\)</span> while the second covariance matrix (<span class="math inline">\(\Gamma\)</span>). describes the covariance between the columns of <span class="math inline">\(\Lambda\)</span>.</p>
<p>The relationship between the multivariate normal and the matrix normal is as follows. <span class="math display">\[\Lambda \sim N(\Theta, \Sigma, \Gamma) \leftrightarrow vec(\Lambda) \sim N(vec(\Theta), \Gamma \otimes \Sigma)\]</span> where <span class="math inline">\(\otimes\)</span> represents the Kronecker product and <span class="math inline">\(vec\)</span> represents the vectorization operation (i.e., column stacking of a matrix to produce a very long vector).</p>
<p>So we can now ask, what is the distribution of a single element of <span class="math inline">\(\Lambda\)</span>? The answer is simply <span class="math display">\[\Lambda_{ij} \sim N(\Theta_{ij}, \Sigma_{ii}\Gamma_{jj}).\]</span> Similarly, we can ask about the distribution of a single column of <span class="math inline">\(\Lambda\)</span>: <span class="math display">\[\Lambda_{\cdot j} \sim N(\Theta_{\cdot j}, \Gamma_{jj} \Sigma).\]</span> Make sense? If not take a look at <a href="https://en.wikipedia.org/wiki/Matrix_normal_distribution">wikipedia for a more complete treatment of the matrix-normal</a>.</p>
</div>
</div>
<div id="the-prior-for-sigma" class="section level1">
<h1 class="hasAnchor">
<a href="#the-prior-for-sigma" class="anchor"></a>The prior for <span class="math inline">\(\Sigma\)</span>
</h1>
<p><span class="math inline">\(\Sigma\)</span> describes the covariance between log-ratios. So if <span class="math inline">\(\phi^{-1}\)</span> is the inverse of the <span class="math inline">\(ALR_D\)</span> transform then <span class="math inline">\(\Sigma\)</span> describes the covariance between <span class="math inline">\(ALR_D\)</span> coordinates. Also note, this section is going to be the hardest one, the other priors components will be faster to describe and probably easier to understand.</p>
<div id="background-on-the-prior" class="section level2">
<h2 class="hasAnchor">
<a href="#background-on-the-prior" class="anchor"></a>Background on the Prior</h2>
<p>The prior for <span class="math inline">\(\Sigma\)</span> is an <a href="https://en.wikipedia.org/wiki/Inverse-Wishart_distribution">Inverse Wishart</a> written <span class="math display">\[\Sigma \sim W^{-1}(\Xi, \upsilon)\]</span> where <span class="math inline">\(\Psi\)</span> is called the scale matrix (and must be a valid covariance matrix itself), and <span class="math inline">\(\upsilon\)</span> is called the degrees of freedom parameter. If <span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\((D-1)x(D-1)\)</span> matrix, then there is a constraint on <span class="math inline">\(\upsilon\)</span> such that <span class="math inline">\(\upsilon \geq D-1\)</span>.</p>
<p>The inverse Wishart has a mildly complex form for its moments (e.g., mean and variance). Its mean is given by <span class="math display">\[E[\Sigma] = \frac{\Psi}{\upsilon-D-2} \quad \text{for } \upsilon &gt; D.\]</span> Its variance is somewhat complicated (<a href="https://en.wikipedia.org/wiki/Inverse-Wishart_distribution#Moments">Wikipedia gives the relationships</a>) but for most purposes you can think of <span class="math inline">\(\upsilon\)</span> as setting the variance, larger <span class="math inline">\(\upsilon\)</span> means less uncertainty (lower variance) about the mean, smaller <span class="math inline">\(\upsilon\)</span> means more uncertainty (higher variance) about the mean.</p>
</div>
<div id="choosing-upsilon-and-xi-" class="section level2">
<h2 class="hasAnchor">
<a href="#choosing-upsilon-and-xi-" class="anchor"></a>Choosing <span class="math inline">\(\upsilon\)</span> and <span class="math inline">\(\Xi\)</span>.</h2>
<p>Reading the above may seem intimidating: that’s the form of the mean… so what? What’s-more how should I think about covariance between log-ratios? Here’s how I think about it. I think about it in-terms of putting a prior on the true abundances in log-space and then transforming that into a prior on log-ratios. Before I can really explain that I need to explain a bit more background.</p>
<p><strong>Compositional Data Analysis in a Nutshell</strong> It turns out that those transforms <span class="math inline">\(\phi\)</span> are all examples of log-ratio transforms studied in a field called compositional data analysis. Briefly, all of those transforms can be written in a form: <span class="math inline">\(\eta = \Psi \log \pi\)</span>. So log-ratios (<span class="math inline">\(\eta\)</span>) are just a linear transform of log-transformed relative-abundances. It turns out that because of special properties of <span class="math inline">\(\Psi\)</span>, the following also holds: <span class="math inline">\(\eta = \Psi \log w\)</span> where <span class="math inline">\(w\)</span> are the absolute (not relative) abundances. So we can say that log-ratios are also just a linear transform of log-transformed absolute-abundances.</p>
<p><strong>Linear Transformations of Covariance Matricies</strong> Recall that if <span class="math inline">\(x \sim N(\mu, \Sigma)\)</span> (for multivariate <span class="math inline">\(x\)</span>) then for a matrix <span class="math inline">\(\Psi\)</span> we have <span class="math inline">\(\Psi x \sim N(\Psi \mu, \Psi \Sigma \Psi^T)\)</span>. This is to say that you should think of linear transformations of covariance matrices as being applied by pre <em>and post</em> multiplying by the transformation matrix <span class="math inline">\(\Psi\)</span>.</p>
<p><strong>Linear transformation of the Inverse Wishart</strong> It turns out that if we have <span class="math inline">\(\Omega \sim W^{-1}(\gamma, S)\)</span> for a <span class="math inline">\(D\times D\)</span> covariance matrix <span class="math inline">\(\Omega\)</span> then for <span class="math inline">\(M\times D\)</span> matrix <span class="math inline">\(\Psi\)</span> we have <span class="math inline">\(\Psi \Omega \Psi^T \sim W^{-1}(\upsilon, \Psi S \Psi^T)\)</span>.</p>
<p><strong>Putting It All Together</strong> A central question: what is a reasonable prior for log-ratios? We are not used to working with log-ratios so this is difficult. A potentially simpler problem is to place a prior on the log-absolute-abundances (<span class="math inline">\(\Omega\)</span>) of whatever we are measuring, <em>e.g.</em>, placing a prior on the covariance between log-absolute-abundances of bacteria (<span class="math inline">\(\Omega \sim W^{-1}(\gamma, S)\)</span>.</p>
<p><em>An example:</em> Lets say that for a given microbiome dataset, I have weak prior belief that, on average, all the taxa are independent with variance 1. I want to come up with values <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(S\)</span> for my prior on <span class="math inline">\(\Omega\)</span> that reflect this. Lets start by specifying the mean for <span class="math inline">\(\Omega\)</span>. <span class="math display">\[E[\Omega] =I_D.\]</span> Next we say we have little certainty about this mean (want high variance) so we set <span class="math inline">\(\gamma\)</span> to be close to the lower bound of <span class="math inline">\(D\)</span> (I often like <span class="math inline">\(\gamma=D+3\)</span>). Now we have <span class="math inline">\(\gamma\)</span> we need to calculate <span class="math inline">\(S\)</span> which we do by solving for <span class="math inline">\(S\)</span> in the equation for the Inverse-Wishart mean<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>: <span class="math display">\[S = E[\Omega](\gamma -D-1).\]</span> There you go that’s a prior on the log-absolute-abundances. Next we need to transform this into a prior on log-ratios. Well the above allows us to do this by simplifying taking the contrast matrix <span class="math inline">\(\Psi\)</span> from the log-ratio transform we want and transforming our prior for <span class="math inline">\(\Omega\)</span> as <span class="math inline">\(\Sigma \sim W^{-1}(\gamma, \Psi S \Psi^T)\)</span>. That’s it there the prior on log-ratios built form a prior on log-absolute-abundances.</p>
<p><em>A Note on Phylogenetic priors:</em> As in phylogenetic linear models, you can make <span class="math inline">\(S\)</span> (as defined above) a covariance derived from the phylogenetic differences between taxa. This allows you to fit phylogenetic linear models in <em>stray</em>.</p>
<p><strong>Making It Even Simpler</strong> Say that you have a prior <span class="math inline">\(\Omega \sim W^{-1}(\gamma, S)\)</span> for covariance between log-absolute-abundances (created as in our example above). You want to transform this into a prior <span class="math inline">\(\Sigma \sim W^{-1}(\upsilon, \Xi)\)</span>. You do this by simply taking <span class="math inline">\(\upsilon=\gamma\)</span>. To calculate <span class="math inline">\(\Xi\)</span>, rather than worrying about <span class="math inline">\(\Psi\)</span>, functions in the <a href="https://jsilve24.github.io/driver/"><em>driver</em> package I wrote</a> will do this for you, here are recipes:</p>
<div class="sourceCode" id="cb1"><html><body><pre class="r"><span class="co"># To put prior on ALR_j coordinates for some j in (1,...,D-1)</span>
<span class="no">Xi</span> <span class="kw">&lt;-</span> <span class="fu">clrvar2alrvar</span>(<span class="no">S</span>, <span class="no">j</span>)

<span class="co"># To put prior in a particular ILR coordinate defined by contrast matrix V</span>
<span class="no">Xi</span> <span class="kw">&lt;-</span> <span class="fu">clrvar2ilrvar</span>(<span class="no">S</span>, <span class="no">V</span>)

<span class="co"># To put prior in CLR coordinates (this one needs two transforms)</span>
<span class="no">foo</span> <span class="kw">&lt;-</span> <span class="fu">clrvar2alrvar</span>(<span class="no">S</span>, <span class="no">D</span>)
<span class="no">Xi</span> <span class="kw">&lt;-</span> <span class="fu">alrvar2clrvar</span>(<span class="no">foo</span>, <span class="no">D</span>)</pre></body></html></div>
<p>Hopefully that is simple enough to be useful for folks.</p>
</div>
</div>
<div id="the-prior-for-lambda" class="section level1">
<h1 class="hasAnchor">
<a href="#the-prior-for-lambda" class="anchor"></a>The prior for <span class="math inline">\(\Lambda\)</span>
</h1>
<p><span class="math inline">\(\Lambda\)</span> are the regression parameters in the linear model. The prior for <span class="math inline">\(\Lambda\)</span> is just a matrix-normal which we described above: <span class="math display">\[\Lambda \sim N(\Theta, \Sigma, \Gamma).\]</span> Here <span class="math inline">\(\Theta\)</span> is the mean matrix of <span class="math inline">\(\Lambda\)</span>, <span class="math inline">\(\Sigma\)</span> is actually random (i.e., you don’t have to specify it, its specified by the prior on <span class="math inline">\(\Sigma\)</span> we discussed already), and <span class="math inline">\(\Gamma\)</span> is a <span class="math inline">\(QxQ\)</span><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> covariance matrix describing the covariance between the columns of <span class="math inline">\(\Lambda\)</span> (<em>i.e.</em>, between the effect of the different covariates). So we really need to just discuss specifying <span class="math inline">\(\Theta\)</span> and specifying <span class="math inline">\(\Gamma\)</span>.</p>
<div id="choosing-theta" class="section level2">
<h2 class="hasAnchor">
<a href="#choosing-theta" class="anchor"></a>Choosing <span class="math inline">\(\Theta\)</span>
</h2>
<p>This is really easy, in most situations this will simply be a matrix of zeros. This implies that you expect that on average, the covariates of interest are not associated with composition.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. This helps prevent you from inferrign an effect if there isn’t one.</p>
<p>Outside of this simple case lets say you actually have prior knowledge about the effects of the covariates. Perhaps you have some knowledge about the mean effect of covariates on log-absolute-abundances which you describe in a <span class="math inline">\(D\times Q\)</span> matrix <span class="math inline">\(A\)</span>. Well you can just transform that prior into the log-ratio coordinates you want as follows:</p>
<div class="sourceCode" id="cb2"><html><body><pre class="r"><span class="co"># Transform from log-absolute-abundance effects to effects on absolute-abundances</span>
<span class="no">foo</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span>(<span class="no">A</span>)

<span class="co"># To put prior on ALR_j coordinates for some j in (1,...,D-1)</span>
<span class="no">Theta</span> <span class="kw">&lt;-</span> <span class="kw pkg">driver</span><span class="kw ns">::</span><span class="fu"><a href="https://rdrr.io/pkg/driver/man/array_lr_transforms.html">alr_array</a></span>(<span class="no">foo</span>, <span class="no">j</span>, <span class="kw">parts</span><span class="kw">=</span><span class="fl">1</span>)

<span class="co"># To put prior in a particular ILR coordinate defined by contrast matrix V</span>
<span class="no">Theta</span> <span class="kw">&lt;-</span> <span class="kw pkg">driver</span><span class="kw ns">::</span><span class="fu"><a href="https://rdrr.io/pkg/driver/man/array_lr_transforms.html">ilr_array</a></span>(<span class="no">foo</span>, <span class="no">V</span>, <span class="kw">parts</span><span class="kw">=</span><span class="fl">1</span>)

<span class="co"># To put prior in CLR coordinates</span>
<span class="no">Theta</span> <span class="kw">&lt;-</span> <span class="kw pkg">driver</span><span class="kw ns">::</span><span class="fu"><a href="https://rdrr.io/pkg/driver/man/array_lr_transforms.html">clr_array</a></span>(<span class="no">foo</span>, <span class="kw">parts</span><span class="kw">=</span><span class="fl">1</span>)</pre></body></html></div>
</div>
<div id="choosing-gamma" class="section level2">
<h2 class="hasAnchor">
<a href="#choosing-gamma" class="anchor"></a>Choosing <span class="math inline">\(\Gamma\)</span>
</h2>
<p>Alright, here we get a break as <span class="math inline">\(\Gamma\)</span> doesn’t care what log-ratio coordinates your in. It’s just a <span class="math inline">\(Q\times Q\)</span> covariance matrix describing the covariation between the effects of the <span class="math inline">\(Q\)</span> covariates.</p>
<p>For example, Lets say your data is a microbiome survey of a disease with a number of healthy controls. Your goal is to figure out what is different between the composition of these two groups. Your model may have two covariates, an intercept and a binary variable (1 if sample is from disease, 0 if from healthy). We probably want to set a prior that allows the intercept to be moderately large but we likely believe that the differences between disease and health are small (so we want the effect of the binary covariate to be modest). We could specify: <span class="math display">\[\Gamma = \alpha\begin{bmatrix} 1 &amp; 0 \\ 0&amp; .2 \end{bmatrix}\]</span> for a scalar <span class="math inline">\(\alpha\)</span> which I will discuss in depth below. Note here the off diagonals being zero also specifies that we don’t think there is any covariation between the intercept and the effect of the disease state (probably a pretty good assumption in this example).</p>
<p>The choice of alpha can be important. I will describe it later in the section on how the choice of <span class="math inline">\(\upsilon\)</span> and <span class="math inline">\(\Xi\)</span> interact with the choice of <span class="math inline">\(\Gamma\)</span>. First I need to briefly describe the prior on <span class="math inline">\(\eta\)</span>.</p>
</div>
</div>
<div id="the-prior-for-eta" class="section level1">
<h1 class="hasAnchor">
<a href="#the-prior-for-eta" class="anchor"></a>The Prior for <span class="math inline">\(\eta\)</span>
</h1>
<p><span class="math inline">\(\eta\)</span> are log-ratios from the regression relationship obscured by noise. <span class="math display">\[\eta_j \sim N(\Lambda X_j, \Sigma).\]</span> Notice <span class="math inline">\(\Sigma\)</span> shows up again like it did in the prior for <span class="math inline">\(\Lambda\)</span>. Actually, there are no more parameters we need to specify, the prior for <span class="math inline">\(\eta\)</span> is completely induced based on our priors for <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(\Sigma\)</span>. The reason I discuss it here is that I want readers to recognize that the variation of <span class="math inline">\(\eta\)</span> about the regression relationship is specified by <span class="math inline">\(\Sigma\)</span>. That means that if <span class="math inline">\(\Sigma\)</span> is large there is more noise, small there is less noise. This should also be taken into account when specifying <span class="math inline">\(\upsilon\)</span> and <span class="math inline">\(\Xi\)</span>. The next section will further expand on this idea.</p>
</div>
<div id="how-the-choice-of-upsilon-and-xi-interacts-with-the-choice-of-gamma" class="section level1">
<h1 class="hasAnchor">
<a href="#how-the-choice-of-upsilon-and-xi-interacts-with-the-choice-of-gamma" class="anchor"></a>How the Choice of <span class="math inline">\(\upsilon\)</span> and <span class="math inline">\(\Xi\)</span> Interacts With the Choice of <span class="math inline">\(\Gamma\)</span>
</h1>
<p>The point of this subsection is the following, the choice of <span class="math inline">\(\Gamma\)</span>, <span class="math inline">\(\Xi\)</span>, and <span class="math inline">\(\upsilon\)</span> in some senses place a prior on the signal-to-noise ratio in the data. In short: The larger <span class="math inline">\(\Gamma\)</span> is relative to <span class="math inline">\(\Sigma\)</span> (specified by <span class="math inline">\(\upsilon\)</span> and <span class="math inline">\(\Xi\)</span>) the more signal, the smaller <span class="math inline">\(\Gamma\)</span> is realtive to <span class="math inline">\(\Sigma\)</span> the more noise. I will describe this below.</p>
<p>Notice that we could alternatively write the prior for <span class="math inline">\(\eta\)</span> as <span class="math display">\[\eta \sim N(\Lambda X, \Sigma, I)\]</span> using the matrix normal in parallel to our prior for <span class="math inline">\(\Lambda\)</span> <span class="math display">\[\Lambda \sim N(\Theta, \Sigma, \Gamma).\]</span> We can write the <em>vec</em> form of these relationships as <span class="math display">\[
\begin{align}
vec(\eta) &amp;\sim N(vec(\Lambda X), I\otimes\Sigma) \\
vec(\Lambda) &amp;\sim N(vec(\Theta), \Gamma \otimes \Sigma).
\end{align}
\]</span> If we write <span class="math inline">\(\Gamma\)</span> as the multiplication of a scalar and scaled matrix (a matrix scaled so that the sum of the diagonals equals 1) <span class="math inline">\(\Gamma=\alpha \bar{\Gamma}\)</span> as we did when describing the choice of <span class="math inline">\(\Gamma\)</span> above, then the above equations turn into: <span class="math display">\[
\begin{align}
vec(\eta) &amp;\sim N(vec(\Lambda X), 1(I\otimes\Sigma)) \\
vec(\Lambda) &amp;\sim N(vec(\Theta), \alpha(\bar{\Gamma}\otimes \Sigma)).
\end{align}
\]</span> and we can see that the magnitude of <span class="math inline">\(\Lambda\)</span> is a factor of <span class="math inline">\(\alpha\)</span> times the noise level. If <span class="math inline">\(\alpha&lt;1\)</span> we have that the the magnitude of <span class="math inline">\(\Lambda\)</span> is smaller than the magnitude of the noise. If <span class="math inline">\(\alpha &gt; 1\)</span> we have that the magnitude of <span class="math inline">\(\Lambda\)</span> is greater than the magnitude of the noise.</p>
<p>The actual “signal” is the product <span class="math inline">\(\Lambda X\)</span> (so it depends on the scale of <span class="math inline">\(X\)</span>) as well but hopefully the point is clear: <strong>The magnitude of <span class="math inline">\(\Sigma\)</span> (which is specified by <span class="math inline">\(\upsilon\)</span> and <span class="math inline">\(\Xi\)</span>) in comparision to the magnitude of <span class="math inline">\(\Gamma\)</span> sets the signal-to-noise ratio in our prior.</strong></p>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Note its <span class="math inline">\(D-1\)</span> here because <span class="math inline">\(\Omega\)</span> is <span class="math inline">\(D\times D\)</span> rather than <span class="math inline">\(D-1 \times D-1\)</span><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Where <span class="math inline">\(Q\)</span> is the number of regression covaraites<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>This is the same assumption as used in Ridge Regression<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Justin Silverman.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
